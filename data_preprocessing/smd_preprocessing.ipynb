{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from https://github.com/wagner-d/TimeSeAD/tree/master/timesead\n",
    "Implementation of the Server Machine Dataset [Su2019]_.\n",
    "The data consists of traces from 28 different servers recorded over several weeks. We consider each trace to be a\n",
    "separate dataset.\n",
    "\n",
    ".. note::\n",
    "    Automatically downloading the dataset currently requires that you have `git` installed on your system!\n",
    "\n",
    ".. [Su2019] Y. Su, Y. Zhao, C. Niu, R. Liu, W. Sun, D. Pei.\n",
    "    Robust anomaly detection for multivariate time series through stochastic recurrent neural network.\n",
    "    In: Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining,\n",
    "    2019 Jul 25 (pp. 2828-2837).\n",
    "\"\"\"\n",
    "GITHUB_LINK = 'https://github.com/NetManAIOps/OmniAnomaly.git'\n",
    "\n",
    "FILENAMES = [\n",
    "    'machine-1-1.txt',\n",
    "    'machine-1-2.txt',\n",
    "    'machine-1-3.txt',\n",
    "    'machine-1-4.txt',\n",
    "    'machine-1-5.txt',\n",
    "    'machine-1-6.txt',\n",
    "    'machine-1-7.txt',\n",
    "    'machine-1-8.txt',\n",
    "    'machine-2-1.txt',\n",
    "    'machine-2-2.txt',\n",
    "    'machine-2-3.txt',\n",
    "    'machine-2-4.txt',\n",
    "    'machine-2-5.txt',\n",
    "    'machine-2-6.txt',\n",
    "    'machine-2-7.txt',\n",
    "    'machine-2-8.txt',\n",
    "    'machine-2-9.txt',\n",
    "    'machine-3-1.txt',\n",
    "    'machine-3-10.txt',\n",
    "    'machine-3-11.txt',\n",
    "    'machine-3-2.txt',\n",
    "    'machine-3-3.txt',\n",
    "    'machine-3-4.txt',\n",
    "    'machine-3-5.txt',\n",
    "    'machine-3-6.txt',\n",
    "    'machine-3-7.txt',\n",
    "    'machine-3-8.txt',\n",
    "    'machine-3-9.txt'\n",
    "]\n",
    "\n",
    "# FILENAMES = [\n",
    "#     'machine-1-1.txt'\n",
    "# ]\n",
    "\n",
    "TRAIN_LENS = [28479, 23694, 23702, 23706, 23705, 23688, 23697, 23698, 23693, 23699, 23688, 23689, 23688, 28743, 23696,\n",
    "              23702, 28722, 28700, 23692, 28695, 23702, 23703, 23687, 23690, 28726, 28705, 28703, 28713]\n",
    "\n",
    "TEST_LENS = [28479, 23694, 23703, 23707, 23706, 23689, 23697, 23699, 23694, 23700, 23689, 23689, 23689, 28743, 23696,\n",
    "             23703, 28722, 28700, 23693, 28696, 23703, 23703, 23687, 23691, 28726, 28705, 28704, 28713]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def save_statistics(frame: pd.DataFrame, path: str):\n",
    "    \"\"\"\n",
    "    Compute feature-wise mean, standard deviation, minimum, and maximum values for a dataset consisting of a single\n",
    "    :class:`~pandas.DataFrame` and save them as a `.npz` file.\n",
    "\n",
    "    :param frame: The dataset for which to compute and save statistics.\n",
    "    :param path: Path to save the statistics via :func:`numpy.savez`.\n",
    "    \"\"\"\n",
    "    mean = frame.mean().to_numpy()\n",
    "    std = frame.std().to_numpy()\n",
    "    min = frame.min().to_numpy()\n",
    "    max = frame.max().to_numpy()\n",
    "    median = frame.median().to_numpy()\n",
    "\n",
    "    np.savez(path, mean=mean, std=std, min=min, max=max, median=median)\n",
    "\n",
    "# preprocess data\n",
    "def preprocess_smd_data(dataset_dir: str, out_dir: str, filenames: List[str]):\n",
    "    \"\"\"\n",
    "    Preprocess SMD dataset for experiments\n",
    "\n",
    "    :param dataset_dir: Path to the dataset folder\n",
    "    :param out_dir: Directory where the preprocessed data should be saved. This directory should exist already.\n",
    "    \"\"\"\n",
    "    for filename in filenames:\n",
    "        data = np.genfromtxt(os.path.join(dataset_dir, 'train', filename), dtype=np.float32, delimiter=',')\n",
    "        data = pd.DataFrame(data)\n",
    "\n",
    "        file_info = filename.split('.')\n",
    "\n",
    "        # Save dataset statistics\n",
    "        stats_file = os.path.join(out_dir, f'{file_info[0]}_stats.npz')\n",
    "        save_statistics(data, stats_file)\n",
    "\n",
    "smd_path = \"../data/ServerMachineDataset\"\n",
    "processed_dir = os.path.join(smd_path, 'processed')\n",
    "os.makedirs(processed_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:26<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of isolated anomaly clips: 10\n",
      "Number of clips in train set: 6328\n",
      "Percent anomaly in train set: 5.847029077117573%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:12<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of isolated anomaly clips: 88\n",
      "Number of clips in test set: 728\n",
      "Percent anomaly in test set: 12.087912087912088%\n",
      "Difference in mean: [-5.29941022e-02 -5.60901910e-02 -6.14546984e-02 -6.89448342e-02\n",
      "  1.36468530e-01 -7.22491741e-03 -1.09375715e-02  0.00000000e+00\n",
      " -5.20095229e-03 -5.40256547e-03 -2.21763067e-02  1.00264698e-03\n",
      " -2.26004291e-02 -1.06054172e-02 -6.77057728e-03 -1.62157193e-02\n",
      " -1.33024834e-04 -1.79709605e-05 -4.10780907e-02 -2.99901217e-02\n",
      " -3.79961729e-02 -2.76627243e-02 -2.79120654e-02 -2.65056789e-02\n",
      "  2.46123299e-02 -6.28837943e-03  2.23241095e-05 -3.53244096e-02\n",
      "  4.26945808e-05  4.50034440e-03 -2.42450088e-02  1.23772025e-02\n",
      " -1.90595910e-03  2.15535238e-03 -9.53087211e-03 -1.47540569e-02\n",
      " -5.37676364e-03  8.36248510e-04]\n",
      "Difference in std: [-0.04996069 -0.05103254 -0.05180858 -0.04964791  0.11900964  0.00413758\n",
      "  0.01541451  0.         -0.00886978 -0.0242696  -0.02245094 -0.00131411\n",
      " -0.09674101 -0.0125635  -0.0066585  -0.0275014  -0.00419776 -0.00101301\n",
      " -0.02577475 -0.01682051 -0.02509667 -0.02019148 -0.0334858  -0.03860304\n",
      "  0.01139319 -0.02299166  0.00386575 -0.02468552  0.00399398 -0.01984498\n",
      " -0.01758656  0.01983708  0.00723117  0.00057089 -0.0250957  -0.02344956\n",
      " -0.01280444 -0.00137751]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_data_to_clips(smd_path, test_str, clip_len=720, test_split=14400, scaler=None, shots=-1):\n",
    "    \"\"\"\n",
    "    Read a single dataset from the SMD dataset.\n",
    "\n",
    "    :param smd_path: Path to the SMD dataset folder.\n",
    "    :param test_str: Either 'train' or 'test'.\n",
    "    :param clip_len: Length of each clip (in minutes)\n",
    "    \"\"\"\n",
    "    output_file = os.path.join(smd_path, \"smd_\" + test_str + \".h5\")\n",
    "\n",
    "    f = h5py.File(output_file, \"w\")\n",
    "    f.close()\n",
    "\n",
    "    all_clips = []\n",
    "    all_targets = []\n",
    "\n",
    "    for filename in tqdm(FILENAMES):\n",
    "        if test_str == 'train':\n",
    "            data = np.genfromtxt(os.path.join(smd_path, test_str, filename), dtype=np.float32, delimiter=',')\n",
    "            target = np.zeros(data.shape[0])\n",
    "            test_data = np.genfromtxt(os.path.join(smd_path, 'test', filename), dtype=np.float32, delimiter=',')[0:test_split]\n",
    "            data = np.concatenate((data, test_data))\n",
    "            test_target = np.genfromtxt(os.path.join(smd_path, 'test_label', filename), dtype=np.float32, delimiter=',')[0:test_split]\n",
    "            target = np.concatenate((target, test_target))\n",
    "        else:\n",
    "            data = np.genfromtxt(os.path.join(smd_path, test_str, filename), dtype=np.float32, delimiter=',')[test_split:]\n",
    "            target = np.genfromtxt(os.path.join(smd_path, 'test_label', filename), dtype=np.float32, delimiter=',')[test_split:]\n",
    "\n",
    "        # Split data into clips\n",
    "        num_clips = int(np.floor(data.shape[0] / clip_len))\n",
    "\n",
    "        for i in range(num_clips):\n",
    "            clip = data[i * clip_len:(i + 1) * clip_len, :]\n",
    "            all_clips.append(clip)\n",
    "            all_targets.append(int(any(target[i * clip_len:(i + 1) * clip_len])))\n",
    "\n",
    "    data = np.stack(all_clips, axis=0)\n",
    "    mean = np.mean(data, axis=(0,1))\n",
    "    std = np.std(data, axis=(0,1))\n",
    "\n",
    "    # Normalize data\n",
    "    data = np.reshape(data, (-1, 38))\n",
    "    if scaler==None:\n",
    "        scaler = MinMaxScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    data = np.reshape(data, (-1, clip_len, 38))\n",
    "    data = np.transpose(data, (0, 2, 1))\n",
    "\n",
    "    target = np.stack(all_targets, axis=0)\n",
    "\n",
    "    abnormal_data = data[target==1]\n",
    "    abnormal_target = target[target==1]\n",
    "\n",
    "    if shots>0:\n",
    "        abnormal_data = abnormal_data[:shots]\n",
    "        abnormal_target = abnormal_target[:shots]\n",
    "\n",
    "    print(f'Number of isolated anomaly clips: {abnormal_data.shape[0]}')\n",
    "\n",
    "    print(f'Number of clips in {test_str} set: {data.shape[0]}')\n",
    "    print(f'Percent anomaly in {test_str} set: {np.mean(target) * 100}%')\n",
    "    \n",
    "    with h5py.File(output_file, 'w') as hdf:\n",
    "        hdf.create_dataset('X', data=data)\n",
    "        hdf.create_dataset('y', data=target)\n",
    "        hdf.create_dataset('X_anom', data=abnormal_data)\n",
    "        hdf.create_dataset('y_anom', data=abnormal_target)\n",
    "\n",
    "    return scaler, mean, std, data, target\n",
    "\n",
    "scaler, train_mean, train_std, data, target = read_data_to_clips(smd_path, test_str='train', clip_len=200, test_split=20000, shots=10)\n",
    "_, test_mean, test_std, _, _ = read_data_to_clips(smd_path, test_str='test', clip_len=200, test_split=20000, scaler=scaler)\n",
    "\n",
    "print(f'Difference in mean: {train_mean - test_mean}')\n",
    "print(f'Difference in std: {train_std - test_std}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
