{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08abcb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some data preprocessing codes for TUSZ are adapted from https://github.com/tsy935/eeg-gnn-ssl\n",
    "Compatible with TUSZ v2.0.0\n",
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyedflib\n",
    "import h5py\n",
    "from scipy.signal import resample\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "# Channels of interest\n",
    "INCLUDED_CHANNELS = [\n",
    "    'EEG FP1',\n",
    "    'EEG FP2',\n",
    "    'EEG F3',\n",
    "    'EEG F4',\n",
    "    'EEG C3',\n",
    "    'EEG C4',\n",
    "    'EEG P3',\n",
    "    'EEG P4',\n",
    "    'EEG O1',\n",
    "    'EEG O2',\n",
    "    'EEG F7',\n",
    "    'EEG F8',\n",
    "    'EEG T3',\n",
    "    'EEG T4',\n",
    "    'EEG T5',\n",
    "    'EEG T6',\n",
    "    'EEG FZ',\n",
    "    'EEG CZ',\n",
    "    'EEG PZ']\n",
    "\n",
    "# All seizure labels available in TUH\n",
    "ALL_LABEL_DICT = {'fnsz': 1, 'gnsz': 2, 'spsz': 3, 'cpsz': 4,\n",
    "                  'absz': 5, 'tnsz': 6, 'tcsz': 7, 'mysz': 8}\n",
    "\n",
    "# Dataset\n",
    "FREQUENCY = 100 # Hz\n",
    "clip_len = 10\n",
    "time_step = 1\n",
    "is_fft = False\n",
    "# is_fft = True\n",
    "split = \"eval\"\n",
    "# anom_cls_inc = ['fnsz'] \n",
    "anom_cls_inc = []\n",
    "\n",
    "if is_fft:\n",
    "    fft_label='fft'\n",
    "else:\n",
    "    fft_label='nofft'\n",
    "    \n",
    "if anom_cls_inc == []:\n",
    "    anom_cls_label = 'allcs'\n",
    "    anom_cls_idx = [1,2,3,4,5,6,7,8]\n",
    "else:\n",
    "    anom_cls_label = 'inc'\n",
    "    anom_cls_idx = []\n",
    "    for lab in anom_cls_inc:\n",
    "        anom_cls_label = lab[0] + anom_cls_label \n",
    "        anom_cls_idx.append(ALL_LABEL_DICT[lab])\n",
    "\n",
    "# Data paths\n",
    "raw_edf_dir = '../data/TUSZ/eval' # dir of unprecessed data tree \n",
    "save_dir = '../data/TUSZ/eval_resampled' # dir of resampled outputs\n",
    "output_dir = '../data/TUSZ'  # dir of preprocessed outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0679a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:05<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fnsz': 102, 'gnsz': 58, 'spsz': 0, 'cpsz': 31, 'absz': 1, 'tnsz': 1, 'tcsz': 8, 'mysz': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "edf_files = []\n",
    "for path, subdirs, files in os.walk(raw_edf_dir):\n",
    "    for name in files:\n",
    "        if \".edf\" in name:\n",
    "            edf_files.append(os.path.join(path, name))\n",
    "            \n",
    "seizure_class_counts =  {'fnsz': 0, 'gnsz': 0, 'spsz': 0, 'cpsz': 0, \n",
    "                         'absz': 0, 'tnsz': 0, 'tcsz': 0, 'mysz': 0}\n",
    "\n",
    "for cls in tqdm(seizure_class_counts): # for each seizure class\n",
    "    for file_name in edf_files:\n",
    "        csv_file = file_name.split(\".edf\")[0] + \".csv\"\n",
    "        with open(csv_file) as f:\n",
    "            for line in f.readlines():\n",
    "                if \"#\" in line:\n",
    "                    continue\n",
    "                if \"channel,start_time,stop_time,label,confidence\" in line:\n",
    "                    continue\n",
    "                if cls in line: \n",
    "                    seizure_class_counts[cls] += 1\n",
    "                    break\n",
    "\n",
    "print(seizure_class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cdaa0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 Resampling\n",
    "def getEDFsignals(edf):\n",
    "    \"\"\"\n",
    "    Get EEG signal in edf file\n",
    "    Args:\n",
    "        edf: edf object\n",
    "    Returns:\n",
    "        signals: shape (num_channels, num_data_points)\n",
    "    \"\"\"\n",
    "    n = edf.signals_in_file\n",
    "    samples = edf.getNSamples()[0]\n",
    "    signals = np.zeros((n, samples))\n",
    "    for i in range(n):\n",
    "        try:\n",
    "            signals[i, :] = edf.readSignal(i)\n",
    "        except:\n",
    "            pass\n",
    "    return signals\n",
    "\n",
    "def getOrderedChannels(file_name, verbose, labels_object, channel_names):\n",
    "    labels = list(labels_object)\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = labels[i].split(\"-\")[0]\n",
    "\n",
    "    ordered_channels = []\n",
    "    for ch in channel_names:\n",
    "        try:\n",
    "            ordered_channels.append(labels.index(ch))\n",
    "        except:\n",
    "            if verbose:\n",
    "                print(file_name + \" failed to get channel \" + ch)\n",
    "            raise Exception(\"channel not match\")\n",
    "    return ordered_channels\n",
    "\n",
    "def resampleData(signals, to_freq, window_size):\n",
    "    \"\"\"\n",
    "    Resample signals from its original sampling freq to another freq\n",
    "    Args:\n",
    "        signals: EEG signal slice, (num_channels, num_data_points)\n",
    "        to_freq: Re-sampled frequency in Hz\n",
    "        window_size: time window in seconds\n",
    "    Returns:\n",
    "        resampled: (num_channels, resampled_data_points)\n",
    "    \"\"\"\n",
    "    num = int(to_freq * window_size)\n",
    "    resampled = resample(signals, num=num, axis=1)\n",
    "    return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08539374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 881/881 [10:07<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESAMPLING DONE\n",
      "Failed files:  []\n",
      "0 files removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "failed_files = []\n",
    "for idx in tqdm(range(len(edf_files))):\n",
    "    edf_fn = edf_files[idx]\n",
    "    save_fn = os.path.join(save_dir, edf_fn.split(\"\\\\\")[-1].split(\".edf\")[0] + \".h5\")\n",
    "    if os.path.exists(save_fn):\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            f = pyedflib.EdfReader(edf_fn)\n",
    "            orderedChannels = getOrderedChannels(\n",
    "                edf_fn, False, f.getSignalLabels(), INCLUDED_CHANNELS\n",
    "            )\n",
    "            signals = getEDFsignals(f)\n",
    "            signal_array = np.array(signals[orderedChannels, :])\n",
    "\n",
    "            sample_freq = f.getSampleFrequency(0)\n",
    "            if sample_freq != FREQUENCY:\n",
    "                signal_array = resampleData(\n",
    "                    signal_array,\n",
    "                    to_freq=FREQUENCY,\n",
    "                    window_size=int(signal_array.shape[1] / sample_freq),\n",
    "                )\n",
    "\n",
    "            with h5py.File(save_fn, \"w\") as hf:\n",
    "                hf.create_dataset(\"resampled_signal\", data=signal_array)\n",
    "    #             hf.create_dataset(\"resample_freq\", data=FREQUENCY)\n",
    "\n",
    "        except BaseException as e:\n",
    "            failed_files.append(edf_fn)\n",
    "\n",
    "print(\"RESAMPLING DONE\")\n",
    "print(\"Failed files: \",failed_files)\n",
    "num_removed = 0\n",
    "for failed in failed_files:\n",
    "    edf_files.remove(failed)\n",
    "    num_removed += 1\n",
    "print(f'{num_removed} files removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1578b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 Main Data Processing\n",
    "# - Split into clips \n",
    "# - Fft\n",
    "# - Seizure/class labels\n",
    "\n",
    "def getAnomalousChannels(montage, channels):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        montage: string of bipolar montage\n",
    "        channels: list of included channels\n",
    "    Returns:\n",
    "        anom_channels: one hot vector of anomalous channels\n",
    "    \"\"\"\n",
    "    anom_channels = []\n",
    "    for ch in INCLUDED_CHANNELS:\n",
    "        if montage.split(\"-\")[0] in ch:\n",
    "            anom_channels.append(1)\n",
    "        elif montage.split(\"-\")[1] in ch:\n",
    "            anom_channels.append(1)\n",
    "        else: \n",
    "            anom_channels.append(0)\n",
    "            \n",
    "    return anom_channels\n",
    "\n",
    "def getSeizureTimes(file_name, verbose=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        file_name: edf file name\n",
    "    Returns:\n",
    "        seizure_times: list of times of seizure onset in seconds\n",
    "        seizure_classes: list of seizure classes corresponding to seizure onset\n",
    "        seizure_channels: list of seizure channels corresponding to seizure onset\n",
    "    \"\"\"\n",
    "    csv_file = file_name.split(\".edf\")[0] + \".csv\"\n",
    "\n",
    "    seizure_times = []\n",
    "    seizure_classes = []\n",
    "    seizure_channels = []\n",
    "    with open(csv_file) as f:\n",
    "        for line in f.readlines():\n",
    "            if \"#\" in line:\n",
    "                continue\n",
    "            if \"channel,start_time,stop_time,label,confidence\" in line:\n",
    "                continue\n",
    "            seizure_time = [float(line.strip().split(\",\")[1]),float(line.strip().split(\",\")[2])]\n",
    "            for cls in ALL_LABEL_DICT: # for each seizure class\n",
    "                if cls in line: \n",
    "                    # seizure start and end time\n",
    "                    seizure_times.append(seizure_time)\n",
    "                    seizure_classes.append(int(ALL_LABEL_DICT[cls]))\n",
    "                    seizure_channels.append(getAnomalousChannels(line.strip().split(\",\")[0],INCLUDED_CHANNELS))\n",
    "\n",
    "    if verbose:\n",
    "        print(csv_file)\n",
    "        print(seizure_times)\n",
    "        print(seizure_classes)\n",
    "        print(seizure_channels)\n",
    "        print(len(seizure_times))\n",
    "\n",
    "    return seizure_times, seizure_classes, seizure_channels\n",
    "\n",
    "def computeFFT(signals, n):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        signals: EEG signals, (number of channels, number of data points)\n",
    "        n: length of positive frequency terms of fourier transform\n",
    "    Returns:\n",
    "        FT: log amplitude of FFT of signals, (number of channels, number of data points)\n",
    "        P: phase spectrum of FFT of signals, (number of channels, number of data points)\n",
    "    \"\"\"\n",
    "    # fourier transform\n",
    "    fourier_signal = fft(signals, n=n, axis=-1)  # FFT on the last dimension\n",
    "\n",
    "    # only take the positive freq part\n",
    "    idx_pos = int(np.floor(n / 2))\n",
    "    fourier_signal = fourier_signal[:, :idx_pos]\n",
    "    amp = np.abs(fourier_signal)\n",
    "    amp[amp == 0.0] = 1e-8  # avoid log of 0\n",
    "\n",
    "    FT = np.log(amp)\n",
    "    P = np.angle(fourier_signal)\n",
    "\n",
    "    return FT, P\n",
    "\n",
    "def computeSliceMatrix(\n",
    "        h5_fn,\n",
    "        seizure_times,\n",
    "        seizure_classes,\n",
    "        seizure_channels,\n",
    "        clip_idx,\n",
    "        clip_len=12,\n",
    "        time_step_size=1,\n",
    "        is_fft=False,\n",
    "        n_fft=FREQUENCY,\n",
    "        res_freq=FREQUENCY,\n",
    "        inc_chan=INCLUDED_CHANNELS,\n",
    "        verbose=False):\n",
    "    \"\"\"\n",
    "    Convert entire EEG sequence into clips of length clip_len\n",
    "    Args:\n",
    "        h5_fn: file name of resampled signal h5 file (full path)\n",
    "        clip_idx: index of current clip/sliding window\n",
    "        time_step_size: length of each time_step_size, in seconds, int\n",
    "        clip_len: sliding window size or EEG clip length, in seconds, int\n",
    "        is_fft: whether to perform FFT on raw EEG data\n",
    "        n_fft: number of frequency terms for FFT\n",
    "    Returns:\n",
    "        slices: list of EEG clips, each having shape (num_channels, n_fft/2)\n",
    "        is_seizure: list of seizure labels for each clip, 1 for seizure, 0 for no seizure\n",
    "        is_seizure_one_hot: list of one-hot matrices of each clip that contains anomalous sensors\n",
    "        is_seizure_class: list of classes of each clip, 0 for normal, >0 for seizure\n",
    "        \n",
    "    \"\"\"\n",
    "    with h5py.File(h5_fn, 'r') as f:\n",
    "        signal_array = f[\"resampled_signal\"][()]\n",
    "    \n",
    "    # clip slicing\n",
    "    physical_clip_len = int(res_freq * clip_len)\n",
    "    physical_time_step_size = int(res_freq * time_step_size)\n",
    "    \n",
    "    start_window = clip_idx * physical_clip_len\n",
    "    end_window = start_window + physical_clip_len\n",
    "    window = pd.Interval(start_window, end_window)\n",
    "    # (num_channels, physical_clip_len)\n",
    "    curr_slc = signal_array[:, start_window:end_window]\n",
    "    \n",
    "    start_time_step = 0\n",
    "    time_steps = []\n",
    "    while start_time_step <= curr_slc.shape[1] - physical_time_step_size:\n",
    "        end_time_step = start_time_step + physical_time_step_size\n",
    "        # (num_channels, physical_time_step_size)\n",
    "        curr_time_step = curr_slc[:, start_time_step:end_time_step]\n",
    "        if is_fft:\n",
    "            curr_time_step, _ = computeFFT(\n",
    "                curr_time_step, n=physical_time_step_size)\n",
    "        time_steps.append(curr_time_step)\n",
    "        start_time_step = end_time_step\n",
    "        \n",
    "    eeg_clip = np.concatenate(time_steps, axis=1)\n",
    "    \n",
    "    # determine if there's seizure in current clip\n",
    "    is_seizure = 0\n",
    "    is_seizure_one_hot = np.array([0]*len(inc_chan))\n",
    "    is_seizure_class = []\n",
    "    \n",
    "    for ivl, t in enumerate(seizure_times):\n",
    "        start_t = int(t[0] * res_freq)\n",
    "        end_t = int(t[1] * res_freq)\n",
    "        seizure_ivl = pd.Interval(start_t,end_t)\n",
    "                            \n",
    "        if window.overlaps(seizure_ivl):\n",
    "            is_seizure = 1\n",
    "            is_seizure_one_hot = is_seizure_one_hot + np.array(seizure_channels[ivl])\n",
    "            is_seizure_one_hot = is_seizure_one_hot > 0\n",
    "            is_seizure_one_hot = is_seizure_one_hot.astype(int)\n",
    "            if seizure_classes[ivl] not in is_seizure_class:\n",
    "                is_seizure_class.append(seizure_classes[ivl])\n",
    "        \n",
    "        if is_seizure_class==[]:\n",
    "            is_seizure_class.append(0)\n",
    "\n",
    "    if verbose:\n",
    "        print(clip_idx)\n",
    "        print(window)\n",
    "        print(is_seizure)\n",
    "        print(is_seizure_one_hot)\n",
    "        print(is_seizure_class)\n",
    "        \n",
    "    return eeg_clip, is_seizure, is_seizure_one_hot, is_seizure_class\n",
    "\n",
    "def getNumClips(h5_fn_path,\n",
    "        clip_len=12,\n",
    "        res_freq=FREQUENCY):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        h5_fn_path: file name of resampled signal h5 file (full path)\n",
    "        clip_len: length of each clips (in seconds)\n",
    "        res_freq: sampling frequency\n",
    "    Returns:\n",
    "        num_clips: number of clips available in the file\n",
    "    \"\"\"\n",
    "    physical_clip_len = int(res_freq * clip_len)\n",
    "    \n",
    "    # read file\n",
    "    with h5py.File(h5_fn_path, \"r\") as f:\n",
    "        signal_array = f[\"resampled_signal\"][()]\n",
    "        \n",
    "    # calculate number of clips\n",
    "    num_clips = int(signal_array.shape[1] / physical_clip_len)\n",
    "    \n",
    "    return num_clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5171b01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 881/881 [10:47<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "Abnormal ratio= 0.06785009015197041\n",
      "Abnormal node ratio= 0.03927485279697053\n",
      "Seizure class counts: {'fnsz': 1598, 'gnsz': 1024, 'spsz': 0, 'cpsz': 395, 'absz': 73, 'tnsz': 5, 'tcsz': 68, 'mysz': 0}\n",
      "Seizure node class counts: {'fnsz': 13517, 'gnsz': 15163, 'spsz': 0, 'cpsz': 3933, 'absz': 1241, 'tnsz': 39, 'tcsz': 921, 'mysz': 0}\n",
      "Seizure class ratios: {'fnsz': 0.0342992058381627, 'gnsz': 0.021978965443228162, 'spsz': 0.0, 'cpsz': 0.008478214209057738, 'absz': 0.0015668598411676326, 'tnsz': 0.0001073191672032625, 'tcsz': 0.00145954067396437, 'mysz': 0.0}\n",
      "Seizure node ratios: {'fnsz': 0.01571967186042671, 'gnsz': 0.01763389690165349, 'spsz': 0.0, 'cpsz': 0.0045739046701974, 'absz': 0.001443227992808282, 'tnsz': 4.535527132918856e-05, 'tcsz': 0.0010710821767739144, 'mysz': 0.0}\n",
      "Normal count: 43427\n",
      "Normal node count: 825113\n",
      "Clips excluded: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_file = os.path.join(\n",
    "    output_dir,\n",
    "    split +\n",
    "    \"_\" +\n",
    "    anom_cls_label +\n",
    "    \"_\" +\n",
    "    fft_label +\n",
    "    \".h5\")\n",
    "\n",
    "f = h5py.File(output_file, \"w\")\n",
    "f.close()\n",
    "\n",
    "ds_idx = 0\n",
    "nm_idx = 0\n",
    "am_idx = 0\n",
    "normal_count = 0\n",
    "anom_node_count = 0\n",
    "seizure_clip_counts =  {'fnsz': 0, 'gnsz': 0, 'spsz': 0, 'cpsz': 0, \n",
    "                         'absz': 0, 'tnsz': 0, 'tcsz': 0, 'mysz': 0}\n",
    "seizure_node_counts =  {'fnsz': 0, 'gnsz': 0, 'spsz': 0, 'cpsz': 0, \n",
    "                         'absz': 0, 'tnsz': 0, 'tcsz': 0, 'mysz': 0}\n",
    "clips_excluded = 0\n",
    "\n",
    "# for idx in tqdm(range(100)):\n",
    "for idx in tqdm(range(len(edf_files))):\n",
    "    h5_fn = edf_files[idx].split(\"\\\\\")[-1].split('.edf')[0] + '.h5'\n",
    "    h5_fn_path = os.path.join(save_dir,h5_fn)\n",
    "    edf_fn = edf_files[idx]\n",
    "    verbosity = False\n",
    "    \n",
    "    num_clips = getNumClips(h5_fn_path,clip_len=clip_len,res_freq=FREQUENCY)\n",
    "    seizure_times, seizure_classes, seizure_channels = getSeizureTimes(edf_fn.split('.edf')[0], verbose=verbosity)\n",
    "    \n",
    "    for clp in range(num_clips):\n",
    "        eeg_clip, is_seizure, is_seizure_one_hot, is_seizure_class = computeSliceMatrix(\n",
    "            h5_fn=h5_fn_path,\n",
    "            seizure_times=seizure_times,\n",
    "            seizure_classes=seizure_classes,\n",
    "            seizure_channels=seizure_channels,\n",
    "            clip_idx=clp,\n",
    "            clip_len=clip_len,\n",
    "            time_step_size=time_step,\n",
    "            is_fft=is_fft,\n",
    "            n_fft=FREQUENCY,\n",
    "            res_freq=FREQUENCY,\n",
    "            inc_chan=INCLUDED_CHANNELS,\n",
    "            verbose=verbosity)\n",
    "        \n",
    "        for cls in is_seizure_class:\n",
    "            if cls>0 and cls not in anom_cls_idx:\n",
    "                clips_excluded += 1\n",
    "                continue\n",
    "        \n",
    "        with h5py.File(output_file,'r+') as hf: # normal + abnormal\n",
    "            grp = hf.create_group(\"clip\"+str(ds_idx))\n",
    "            grp.create_dataset(\"data\",data=eeg_clip)\n",
    "            grp.create_dataset(\"anom_label\",data=is_seizure)\n",
    "            grp.create_dataset(\"anom_class\",data=is_seizure_class)\n",
    "            grp.create_dataset(\"anom_channels\",data=is_seizure_one_hot)\n",
    "\n",
    "\n",
    "        ds_idx += 1\n",
    "        anom_node_count += np.sum(is_seizure_one_hot)\n",
    "\n",
    "        if is_seizure==0:\n",
    "            normal_count += 1\n",
    "            \n",
    "        for cls in seizure_clip_counts:\n",
    "            cls_label = ALL_LABEL_DICT[cls]\n",
    "            if cls_label in is_seizure_class:\n",
    "                seizure_clip_counts[cls] += 1\n",
    "                seizure_node_counts[cls] += sum(is_seizure_one_hot)\n",
    "\n",
    "anom_clips = 0\n",
    "for key in seizure_clip_counts:\n",
    "    anom_clips += seizure_clip_counts[key]\n",
    "seizure_clip_ratios = {key: seizure_clip_counts[key] / (anom_clips+normal_count) for key in seizure_clip_counts}\n",
    "seizure_node_ratios = {key: seizure_node_counts[key] / (anom_node_count + normal_count*19) for key in seizure_node_counts}\n",
    "\n",
    "print(\"DONE\")\n",
    "print(\"Abnormal ratio= {}\".format(1-normal_count/(ds_idx+1)))\n",
    "print(\"Abnormal node ratio= {}\".format(anom_node_count/((ds_idx+1)*len(INCLUDED_CHANNELS))))\n",
    "print(\"Seizure class counts: {}\".format(seizure_clip_counts))\n",
    "print(\"Seizure node class counts: {}\".format(seizure_node_counts))\n",
    "print(\"Seizure class ratios: {}\".format(seizure_clip_ratios))\n",
    "print(\"Seizure node ratios: {}\".format(seizure_node_ratios))\n",
    "print(\"Normal count: {}\".format(normal_count))\n",
    "print(\"Normal node count: {}\".format(normal_count*19))\n",
    "print(\"Clips excluded: {}\".format(clips_excluded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "785f8874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46587it [00:20, 2229.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.697534735305587\n",
      "1.3957092773450142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3 Calculate mean and std\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "output_file = os.path.join(\n",
    "    output_dir,\n",
    "    split +\n",
    "    \"_\" +\n",
    "    anom_cls_label +\n",
    "    \"_\" +\n",
    "    fft_label +\n",
    "    \".h5\")\n",
    "\n",
    "with h5py.File(output_file, \"r\") as f:\n",
    "    keys = list(f.keys())\n",
    "\n",
    "    for i, key in tqdm(enumerate(keys)):\n",
    "        data = f[key]['data'][()]\n",
    "        data = np.reshape(data, (-1,1))\n",
    "        scaler.partial_fit(data)\n",
    "\n",
    "means = scaler.mean_[0]\n",
    "stds = scaler.scale_[0]\n",
    "\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e49d91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46587it [01:09, 666.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Standardize data\n",
    "output_file = os.path.join(\n",
    "    output_dir,\n",
    "    split +\n",
    "    \"_\" +\n",
    "    anom_cls_label +\n",
    "    \"_\" +\n",
    "    fft_label +\n",
    "    \".h5\")\n",
    "\n",
    "output_file_norm = os.path.join(\n",
    "    output_dir,\n",
    "    split +\n",
    "    \"_\" +\n",
    "    anom_cls_label +\n",
    "    \"_\" +\n",
    "    fft_label +\n",
    "    \"_normalized\" +\n",
    "    \".h5\")\n",
    "\n",
    "f = h5py.File(output_file_norm, \"w\")\n",
    "f.close()\n",
    "\n",
    "with h5py.File(output_file, \"r\") as f:\n",
    "    with h5py.File(output_file_norm,'r+') as fn: # normal + abnormal\n",
    "        keys = list(f.keys())\n",
    "\n",
    "        for i, key in tqdm(enumerate(keys)):\n",
    "            x = f[key]['data'][()]\n",
    "            x = np.reshape(x, (-1,1))\n",
    "            x = scaler.transform(x)\n",
    "            x = np.reshape(x, (19,-1))\n",
    "            \n",
    "            grp = fn.create_group(key)\n",
    "            grp.create_dataset(\"data\",data=x)\n",
    "            grp.create_dataset(\"anom_label\",data=f[key]['anom_label'][()])\n",
    "            grp.create_dataset(\"anom_class\",data=f[key]['anom_class'][()])\n",
    "            grp.create_dataset(\"anom_channels\",data=f[key]['anom_channels'][()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed4929c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46587it [00:29, 1575.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46587,)\n",
      "0.06783008135316719\n"
     ]
    }
   ],
   "source": [
    "# Create labels\n",
    "output_file = os.path.join(\n",
    "    output_dir,\n",
    "    split +\n",
    "    \"_\" +\n",
    "    anom_cls_label +\n",
    "    \"_\" +\n",
    "    fft_label +\n",
    "    \"_normalized\" +\n",
    "    \".h5\")\n",
    "\n",
    "with h5py.File(output_file, \"r\") as f:\n",
    "    keys = range(len(f.keys()))\n",
    "    labels = []\n",
    "    cls_labels = []\n",
    "    for i, key in tqdm(enumerate(keys)):\n",
    "        key = \"clip\" + str(key)\n",
    "        labels.append(f[key]['anom_label'][()])\n",
    "        if len(f[key]['anom_class'][()])==1:\n",
    "            cls_labels.append(f[key]['anom_class'][()][0])\n",
    "        else:\n",
    "            cls_labels.append(0)\n",
    "\n",
    "labels = np.array(labels)\n",
    "print(labels.shape)\n",
    "cls_labels = np.array(cls_labels)\n",
    "fnsz_labels = cls_labels==1\n",
    "gnsz_labels = cls_labels==2\n",
    "cpsz_labels = cls_labels==4\n",
    "print(np.mean(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bead0b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "815\n",
      "471\n",
      "208\n",
      "0.01749415072874407\n",
      "0.010110116556120806\n",
      "0.0044647648485629035\n",
      "(46587,)\n",
      "(46587,)\n",
      "(46587,)\n",
      "(46587,)\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(fnsz_labels))\n",
    "print(np.sum(gnsz_labels))\n",
    "print(np.sum(cpsz_labels))\n",
    "print(np.mean(fnsz_labels))\n",
    "print(np.mean(gnsz_labels))\n",
    "print(np.mean(cpsz_labels))\n",
    "print(fnsz_labels.shape)\n",
    "print(gnsz_labels.shape)\n",
    "print(cpsz_labels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c28474bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33421 16042  4216 ... 25709 22472 14133]\n",
      "(41928,)\n",
      "[29744 15218 32651 ... 12666  9081  9430]\n",
      "(4659,)\n",
      "0.067830566685747\n",
      "0.0678257136724619\n",
      "316\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test\n",
    "train_idx, test_idx, train_labels, test_labels = train_test_split(\n",
    "    np.arange(len(labels)), labels, test_size=0.1, random_state=123, stratify=labels\n",
    ")\n",
    "\n",
    "print(train_idx)\n",
    "print(train_idx.shape)\n",
    "print(test_idx)\n",
    "print(test_idx.shape)\n",
    "print(np.mean(train_labels))\n",
    "print(np.mean(test_labels))\n",
    "print(np.sum(test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c88cf6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11477 44255 32456 23248 11131 28450 35694 31883 32130 23769 31894 32575\n",
      " 32486 43285 32611 42076 32862 43127 44256 32098 39658 32060 32059 32049\n",
      " 43053 32296 16444 35701 32339 32345 10194  1884  9673  1452 45548 45749\n",
      " 45538  9247 46019 45578 22634 46002 46024 45561 29867 45967 17362 45752\n",
      " 22642 22644 46026 10050 10095 45571 30032 45994  9207 10197 45978  9819\n",
      " 19878 24308 25289  5125  5290 24451 19626 24519 24305 24988 23957 24599\n",
      " 24456 23967  5827  5284 24452 20272 24590 24596 24528 19932 19751  5828\n",
      " 25229 20185 25109 24598 19881  5128]\n",
      "(90,)\n"
     ]
    }
   ],
   "source": [
    "sz_classes = ['fnsz','gnsz','cpsz']\n",
    "shots_per_class = 30\n",
    "\n",
    "fnsz_train_idx = train_idx[fnsz_labels[train_idx]]\n",
    "gnsz_train_idx = train_idx[gnsz_labels[train_idx]]\n",
    "cpsz_train_idx = train_idx[cpsz_labels[train_idx]]\n",
    "\n",
    "iso_anom_idx = []\n",
    "iso_anom_idx.extend(fnsz_train_idx[:shots_per_class])\n",
    "iso_anom_idx.extend(gnsz_train_idx[:shots_per_class])\n",
    "iso_anom_idx.extend(cpsz_train_idx[:shots_per_class])\n",
    "\n",
    "iso_anom_idx = np.array(iso_anom_idx)\n",
    "print(iso_anom_idx)\n",
    "print(iso_anom_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "528d2348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[11477 44255 32456 23248 11131 28450 35694 31883 32130 23769 31894 32575\n",
      " 32486 43285 32611 42076 32862 43127 44256 32098 39658 32060 32059 32049\n",
      " 43053 32296 16444 35701 32339 32345 10194  1884  9673  1452 45548 45749\n",
      " 45538  9247 46019 45578 22634 46002 46024 45561 29867 45967 17362 45752\n",
      " 22642 22644 46026 10050 10095 45571 30032 45994  9207 10197 45978  9819\n",
      " 19878 24308 25289  5125  5290 24451 19626 24519 24305 24988 23957 24599\n",
      " 24456 23967  5827  5284 24452 20272 24590 24596 24528 19932 19751  5828\n",
      " 25229 20185 25109 24598 19881  5128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:00<00:00, 3102.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 19, 1000)\n",
      "(90,)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "['clip11477', 'clip44255', 'clip32456', 'clip23248', 'clip11131', 'clip28450', 'clip35694', 'clip31883', 'clip32130', 'clip23769', 'clip31894', 'clip32575', 'clip32486', 'clip43285', 'clip32611', 'clip42076', 'clip32862', 'clip43127', 'clip44256', 'clip32098', 'clip39658', 'clip32060', 'clip32059', 'clip32049', 'clip43053', 'clip32296', 'clip16444', 'clip35701', 'clip32339', 'clip32345', 'clip10194', 'clip1884', 'clip9673', 'clip1452', 'clip45548', 'clip45749', 'clip45538', 'clip9247', 'clip46019', 'clip45578', 'clip22634', 'clip46002', 'clip46024', 'clip45561', 'clip29867', 'clip45967', 'clip17362', 'clip45752', 'clip22642', 'clip22644', 'clip46026', 'clip10050', 'clip10095', 'clip45571', 'clip30032', 'clip45994', 'clip9207', 'clip10197', 'clip45978', 'clip9819', 'clip19878', 'clip24308', 'clip25289', 'clip5125', 'clip5290', 'clip24451', 'clip19626', 'clip24519', 'clip24305', 'clip24988', 'clip23957', 'clip24599', 'clip24456', 'clip23967', 'clip5827', 'clip5284', 'clip24452', 'clip20272', 'clip24590', 'clip24596', 'clip24528', 'clip19932', 'clip19751', 'clip5828', 'clip25229', 'clip20185', 'clip25109', 'clip24598', 'clip19881', 'clip5128']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_file_norm = os.path.join(\n",
    "    output_dir,\n",
    "    split +\n",
    "    \"_\" +\n",
    "    anom_cls_label +\n",
    "    \"_\" +\n",
    "    fft_label +\n",
    "    \"_normalized\" +\n",
    "    \".h5\")\n",
    "\n",
    "# Get isolated anomaly data\n",
    "iso_anom_data = []\n",
    "iso_anom_labels = []\n",
    "print(labels)\n",
    "print(labels[iso_anom_idx])\n",
    "print(iso_anom_idx)\n",
    "group_keys = []\n",
    "with h5py.File(output_file_norm, \"r\") as f:\n",
    "    for idx in tqdm(iso_anom_idx):\n",
    "        group_key = \"clip\" + str(idx)\n",
    "        group_keys.append(group_key)\n",
    "        iso_anom_data.append(f[group_key]['data'][()])\n",
    "        iso_anom_labels.append(int(f[group_key]['anom_label'][()]))\n",
    "\n",
    "iso_anom_data = np.stack(iso_anom_data,axis=0)\n",
    "iso_anom_labels = np.array(iso_anom_labels)\n",
    "print(iso_anom_data.shape)\n",
    "print(iso_anom_labels.shape)\n",
    "print(iso_anom_labels)\n",
    "print(group_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d047baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4659/4659 [00:01<00:00, 2976.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4659, 19, 500)\n",
      "(4659,)\n",
      "(4659, 19, 500)\n",
      "(4659,)\n"
     ]
    }
   ],
   "source": [
    "output_file_norm = os.path.join(\n",
    "    output_dir,\n",
    "    split +\n",
    "    \"_\" +\n",
    "    anom_cls_label +\n",
    "    \"_\" +\n",
    "    fft_label +\n",
    "    \"_normalized\" +\n",
    "    \".h5\")\n",
    "\n",
    "# Create test dataset file\n",
    "test_data = []\n",
    "test_labels = []\n",
    "with h5py.File(output_file_norm, \"r\") as f:\n",
    "    for idx in tqdm(test_idx):\n",
    "        group_key = \"clip\" + str(idx)\n",
    "        test_data.append(f[group_key]['data'][()])\n",
    "        test_labels.append(int(f[group_key]['anom_label'][()]))\n",
    "\n",
    "test_data = np.stack(test_data,axis=0)\n",
    "test_labels = np.array(test_labels)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "test_file = os.path.join(output_dir, \"tusz_test_{}.h5\".format(fft_label))\n",
    "with h5py.File(test_file, \"w\") as f:\n",
    "    f.create_dataset(\"X\", data=test_data)\n",
    "    f.create_dataset(\"y\", data=test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60d50911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4659/4659 [00:01<00:00, 2447.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4659, 19, 1000)\n",
      "(4659,)\n",
      "(4659,)\n"
     ]
    }
   ],
   "source": [
    "# Create test dataset file - hard setting\n",
    "output_file_norm = os.path.join(\n",
    "    output_dir,\n",
    "    split +\n",
    "    \"_\" +\n",
    "    anom_cls_label +\n",
    "    \"_\" +\n",
    "    fft_label +\n",
    "    \"_normalized\" +\n",
    "    \".h5\")\n",
    "\n",
    "# Create test dataset file\n",
    "test_data = []\n",
    "test_labels = []\n",
    "test_class_labels = []\n",
    "with h5py.File(output_file_norm, \"r\") as f:\n",
    "    for idx in tqdm(test_idx):\n",
    "        group_key = \"clip\" + str(idx)\n",
    "        test_data.append(f[group_key]['data'][()])\n",
    "        test_labels.append(int(f[group_key]['anom_label'][()]))\n",
    "        cls_label = f[group_key]['anom_class'][()]\n",
    "        # print(cls_label)\n",
    "        if cls_label.size>0:\n",
    "            if cls_label.size==1 and cls_label[0]==0:\n",
    "                test_class_labels.append(0)\n",
    "            else:\n",
    "                cls_label = np.setdiff1d(cls_label,[0])\n",
    "                test_class_labels.append(int(cls_label[0]))\n",
    "        else:\n",
    "            test_class_labels.append(0)\n",
    "            \n",
    "            \n",
    "\n",
    "test_data = np.stack(test_data,axis=0)\n",
    "test_labels = np.array(test_labels)\n",
    "test_class_labels = np.array(test_class_labels)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "print(test_class_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0775cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 2 0 0 2 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 5 0 0 0 0 0 2 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Normal ratio:  0.9321742863275381\n",
      "fnsz ratio:  0.0352006868426701\n",
      "gnsz ratio:  0.0223223867782786\n",
      "spsz ratio:  0.0\n",
      "cpsz ratio:  0.0068684267010088\n",
      "absz ratio:  0.00128783000643915\n",
      "tnsz ratio:  0.000214638334406525\n",
      "tcsz ratio:  0.0019317450096587251\n",
      "mysz ratio:  0.0\n",
      "Abnormal ratio:  0.0678257136724619\n"
     ]
    }
   ],
   "source": [
    "print(test_class_labels[:100])\n",
    "print(\"Normal ratio: \", len(np.where(np.array(test_class_labels) == 0)[0])/len(test_class_labels))\n",
    "print(\"fnsz ratio: \", len(np.where(np.array(test_class_labels) == 1)[0])/len(test_class_labels))\n",
    "print(\"gnsz ratio: \", len(np.where(np.array(test_class_labels) == 2)[0])/len(test_class_labels))\n",
    "print(\"spsz ratio: \", len(np.where(np.array(test_class_labels) == 3)[0])/len(test_class_labels))\n",
    "print(\"cpsz ratio: \", len(np.where(np.array(test_class_labels) == 4)[0])/len(test_class_labels))\n",
    "print(\"absz ratio: \", len(np.where(np.array(test_class_labels) == 5)[0])/len(test_class_labels))\n",
    "print(\"tnsz ratio: \", len(np.where(np.array(test_class_labels) == 6)[0])/len(test_class_labels))\n",
    "print(\"tcsz ratio: \", len(np.where(np.array(test_class_labels) == 7)[0])/len(test_class_labels))\n",
    "print(\"mysz ratio: \", len(np.where(np.array(test_class_labels) == 8)[0])/len(test_class_labels))\n",
    "print(\"Abnormal ratio: \", len(np.where(np.array(test_class_labels) != 0)[0])/len(test_class_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6204513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "4659\n",
      "4343\n",
      "32\n",
      "284\n",
      "seen %: 0.0068684267010088\n",
      "unseen %: 0.060957286971453105\n"
     ]
    }
   ],
   "source": [
    "seen_class = \"cpsz\"\n",
    "\n",
    "class_idx = {'fnsz': 1, 'gnsz': 2, 'cpsz': 4}\n",
    "\n",
    "seen_unseen_labels = []\n",
    "for lbl in test_class_labels:\n",
    "    if lbl == 0:\n",
    "        seen_unseen_labels.append(0)\n",
    "    elif lbl == class_idx[seen_class]:\n",
    "        seen_unseen_labels.append(1)\n",
    "    else:\n",
    "        seen_unseen_labels.append(2)\n",
    "\n",
    "seen_unseen_labels = np.array(seen_unseen_labels)\n",
    "print(seen_unseen_labels)\n",
    "print(len(seen_unseen_labels))\n",
    "print(len(np.where(np.array(seen_unseen_labels) == 0)[0]))\n",
    "print(len(np.where(np.array(seen_unseen_labels) == 1)[0]))\n",
    "print(len(np.where(np.array(seen_unseen_labels) == 2)[0]))\n",
    "print(\"seen %: {}\".format(len(np.where(np.array(seen_unseen_labels) == 1)[0])/len(seen_unseen_labels)))\n",
    "print(\"unseen %: {}\".format(len(np.where(np.array(seen_unseen_labels) == 2)[0])/len(seen_unseen_labels)))\n",
    "\n",
    "test_file = os.path.join(output_dir, \"tusz_test_{}.h5\".format(seen_class))\n",
    "with h5py.File(test_file, \"w\") as f:\n",
    "    f.create_dataset(\"X\", data=test_data)\n",
    "    f.create_dataset(\"y\", data=test_labels)\n",
    "    f.create_dataset(\"seen_unseen_labels\", data=seen_unseen_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c541e3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41928/41928 [00:22<00:00, 1848.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41928, 19, 500)\n",
      "(41928,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:00<00:00, 742.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 19, 500)\n",
      "(90,)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create train dataset file\n",
    "train_data = []\n",
    "train_labels = []\n",
    "with h5py.File(output_file_norm, \"r\") as f:\n",
    "    for idx in tqdm(train_idx):\n",
    "        group_key = \"clip\" + str(idx)\n",
    "        train_data.append(f[group_key]['data'][()])\n",
    "        train_labels.append(int(f[group_key]['anom_label'][()]))\n",
    "\n",
    "train_data = np.stack(train_data,axis=0)\n",
    "train_labels = np.array(train_labels)\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "# Get isolated anomaly data\n",
    "iso_anom_data = []\n",
    "iso_anom_labels = []\n",
    "with h5py.File(output_file_norm, \"r\") as f:\n",
    "    for idx in tqdm(iso_anom_idx):\n",
    "        group_key = \"clip\" + str(idx)\n",
    "        iso_anom_data.append(f[group_key]['data'][()])\n",
    "        iso_anom_labels.append(int(f[group_key]['anom_label'][()]))\n",
    "\n",
    "iso_anom_data = np.stack(iso_anom_data,axis=0)\n",
    "iso_anom_labels = np.array(iso_anom_labels)\n",
    "print(iso_anom_data.shape)\n",
    "print(iso_anom_labels.shape)\n",
    "print(iso_anom_labels)\n",
    "\n",
    "train_file = os.path.join(output_dir, \"tusz_train_{}.h5\".format(fft_label))\n",
    "with h5py.File(train_file, \"w\") as f:\n",
    "    f.create_dataset(\"X\", data=train_data)\n",
    "    f.create_dataset(\"y\", data=train_labels)\n",
    "    f.create_dataset(\"X_anom\", data=iso_anom_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9876f9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41928/41928 [01:01<00:00, 683.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41928, 19, 1000)\n",
      "(41928,)\n"
     ]
    }
   ],
   "source": [
    "# Create train dataset file - hard setting\n",
    "train_data = []\n",
    "train_labels = []\n",
    "with h5py.File(output_file_norm, \"r\") as f:\n",
    "    for idx in tqdm(train_idx):\n",
    "        group_key = \"clip\" + str(idx)\n",
    "        train_data.append(f[group_key]['data'][()])\n",
    "        train_labels.append(int(f[group_key]['anom_label'][()]))\n",
    "\n",
    "train_data = np.stack(train_data,axis=0)\n",
    "train_labels = np.array(train_labels)\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8cade63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19878 24308 25289  5125  5290 24451 19626 24519 24305 24988 23957 24599\n",
      " 24456 23967  5827  5284 24452 20272 24590 24596 24528 19932 19751  5828\n",
      " 25229 20185 25109 24598 19881  5128]\n",
      "(30,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 450.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 19, 1000)\n",
      "(30,)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seen_class = 'cpsz'\n",
    "shots_per_class = 30\n",
    "\n",
    "iso_anom_idx = []\n",
    "if seen_class == 'fnsz':\n",
    "    iso_anom_idx.extend(fnsz_train_idx[:shots_per_class])\n",
    "elif seen_class == 'gnsz':\n",
    "    iso_anom_idx.extend(gnsz_train_idx[:shots_per_class])\n",
    "elif seen_class == 'cpsz':\n",
    "    iso_anom_idx.extend(cpsz_train_idx[:shots_per_class])\n",
    "else: \n",
    "    print('Invalid seen class')\n",
    "\n",
    "iso_anom_idx = np.array(iso_anom_idx)\n",
    "\n",
    "print(iso_anom_idx)\n",
    "print(iso_anom_idx.shape)\n",
    "\n",
    "# Get isolated anomaly data\n",
    "iso_anom_data = []\n",
    "iso_anom_labels = []\n",
    "with h5py.File(output_file_norm, \"r\") as f:\n",
    "    for idx in tqdm(iso_anom_idx):\n",
    "        group_key = \"clip\" + str(idx)\n",
    "        iso_anom_data.append(f[group_key]['data'][()])\n",
    "        iso_anom_labels.append(int(f[group_key]['anom_label'][()]))\n",
    "\n",
    "iso_anom_data = np.stack(iso_anom_data,axis=0)\n",
    "iso_anom_labels = np.array(iso_anom_labels)\n",
    "print(iso_anom_data.shape)\n",
    "print(iso_anom_labels.shape)\n",
    "print(iso_anom_labels)\n",
    "\n",
    "train_file = os.path.join(output_dir, \"tusz_train_{}.h5\".format(seen_class))\n",
    "with h5py.File(train_file, \"w\") as f:\n",
    "    f.create_dataset(\"X\", data=train_data)\n",
    "    f.create_dataset(\"y\", data=train_labels)\n",
    "    f.create_dataset(\"X_anom\", data=iso_anom_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
